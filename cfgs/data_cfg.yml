# Attention! ALL paths must be relative to the 'config.yml' file!

data_path: data/ontonotes5.py
num_proc: 6

# process config
gold_span: False  # if True, we use gold span from OntoNotes5. If False, we get span from scratch by spaCy and stanza parsers
batch_size: 2048 # batch size for data processing
split: test  # train, dev, test
shuffle: True  # whether to shuffle the data
select: 1600  # select a subset of data. If you want to use all data, set select as 0 or empty
save_dir: data/preprocess  # the directory to store the preprocessed data
continue: True  # whether to continue to use the data last time
continue_dir: data/continue  # the directory to store the continued data to be annotated

########################################################
# if gold_span is False, we get span from scratch
# set config as follows

# strict or loose. In strict mode, we get spans based on intersection of spaCy and Stanza results.
# In loose mode, we get spans based on union of spaCy and Stanza results.
cuda_devices: all  # specify which GPU to use, 'all' or comma-split str like '0,1,2,...,n-1' (consume that you have n GPUs)
mode: loose  # strict or loose
span_portion: 0.4  # The proportion of the longest span length to sentence length. To filter out long span.
#batch_num_per_device: 20  # Specify number of batches on each device to be processed in map function. Depends on your RAM memory
#batch_size_per_device: 512  #  Specify batch size per device. Depends on your GPU memory.
spacy_model:
  # https://spacy.io/api/top-level#spacy.load
  # for spacy.load function, we need to specify 'name'
  name: en_core_web_trf  # for English, we can use en_core_web_sm, en_core_web_md, en_core_web_lg, en_core_web_trf
stanza_model:
  # https://stanfordnlp.github.io/stanza/pipeline.html
  lang: en  # for English
  processors: tokenize, ner, pos, constituency  # specify processors, comma-seperated
  # dir: /data1/gcchen/stanza_data  # default ~/stanza_resources

  # tokenizer init
  # https://stanfordnlp.github.io/stanza/tokenize.html#options
  tokenize_pretokenized: True
  tokenize_no_ssplit: True  # https://stanfordnlp.github.io/stanza/tokenize.html#tokenization-without-sentence-segmentation